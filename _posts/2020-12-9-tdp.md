---
layout: post
title: 2020 EMNLP《Exploring Contextualized Neural Language Models for Temporal Dependency Parsing》
categories: [Note, Temporal Relation]
description: 探索时序依存解析的上下文神经语言模型
keywords: Temporal Relation, Temporal Dependency Parsing
mathjax: true
original: true
author: 王亮
authorurl: https://github.com/NeoAlice
---

> 2020 EMNLP《探索时序依存解析的上下文神经语言模型》[(Exploring Contextualized Neural Language Models for Temporal Dependency Parsing)](https://www.aclweb.org/anthology/2020.emnlp-main.689/)的阅读笔记

# **Abstract**

提取事件和时间表达式之间的时序关系是一个具有挑战性的问题，需要句子或篇章级的句法和语义信息，可以由深度上下文语言模型 (LMS) 所捕获，如BERT。本文开发了几种基于 BERT 的时序依存解析器变体，并表明 BERT 显著改善了时序依存解析 (Zhang and Xue, 2018a)。最后详细分析了为什么深度上下文神经 LMS 有帮助，以及它们的不足。Source code：https://github.com/bnmin/tdp_ranking.



# **1 Introduction**

时序依存解析器，将文档中的时间表达式和事件形成一棵时序依存树（TDT）.

例子：

<div style="text-align: center;">
<img src="/images/blog/TDP-2.png" width="350px"/>
</div>

<div style="text-align: center;">
<img src="/images/blog/TDP-1.png" width="350px"/>
</div>



与过去基于 TimeML 成对的时序关系抽取需要 $$\dbinom{n}{2}$$ 对标注相比，TDT 显著降低了标注复杂度且维持了事件的时序关系结构。

BERT 提供了上下文化的词嵌，能够捕获句法和语义信息，恰恰是 TDP 需要的。

本文主要的贡献：

+ 开发了基于 BERT 的时序依存解析器，从直接使用预训练的 BERT 词嵌，到使用 BERT 的多层多头自注意体系结构作为在端到端系统中训练的编码器。
+ 实验显示了基于 BERT 的 TDP 模型的显著优势。与在 (Zhang and Xue, 2018a) 中重新实现神经模型相比，最佳模型实现了13个 F1 点的改进。
+ 详细分析了 BERT 在这项任务中的优势和局限性。



# 2 **BERT-based Neural Models for Temporal Dependency Parsing**

将 TDP 转化为一个排名问题，给定一个已经抽取的孩子 mention $$x_i$$，从根节点中选择一个最合适的父亲 mention, 即来自 $$x_i$$ 周围的窗口 $$x_{i-k},...,x_i,...,x_{i+m}$$ 的 DCT 或 event 或时间表达式以及关系标签 (*before*, *after*, *overlap*, *depends* *on*)。对于窗口中的每一个 $$x_j$$, 模型逐个判断候选亲子对 $$<x_i,x_j>$$。选择分数最高的亲子对且保证不形成环。

<div style="text-align: center;">
<img src="/images/blog/TDP-3.png" width="350px"/>
</div>



# 3 **Experiments**

对于 BILSTM and BILSTM-BERT，在 DCT 孩子节点上的精确率分别是 0.38 和 0.48，而 BERT-FT 则较 BILSTM 提升 0.21 于 DCT 的孩子节点，0.14提升于 事件表达式的孩子节点，0.11 提升于 event 的孩子节点。

<div style="text-align: center;">
<img src="/images/blog/TDP-4.png" width="350px"/>
</div>



# 4  **Analysis**

BERT 的作用：

+ BERT-FT 能够正确地关联在语法上发生在 event 之后的时间表达式，如 *Kuchma and Yeltsin* **signed** *a cooperation plan on* **February 27, 1998**.

  这表明 BERT 可以找到在 child 之后出现的 parent.

+ BERT-FT 能够捕获动词时态，并利用它确定 DCT 和事件链的正确关系。比如一般现在时的动词和 DCT 是 overlap 关系，而过去完成时的动词发生在 DCT 或者与之相邻接的 event 之前。
+ BERT-FT 捕捉具有隐含时序关系的句法结构，如间接引语和动名词。
+ BERT处理句法属性的能力，如嵌入子句，可能使其能够检测连接词的方向，如 since。虽然所有模型都可以将矩阵从句动词识别为正确的父项，但 BERT-FT 更有可能选择正确的标签。 (BILSTM 几乎总是为 DCT 选择“前”，为事件的孩子选择“后”，而忽略了连接词。)
+ BERT-FT 和 BILSTM-BERT 在识别上下文更改（新的“部分”）方面都比 BILSTM 要好得多，并且将这些 event 与DCT 而不是在前几节中的时间表达式连接起来(DCT 孩子的分数证明了这一点)。 由 于BERT 的词嵌入使用句子作为上下文，使用 BERT 的模型可能能够“比较”句子，并判断它们是无关的，尽管它们是相邻的。

**Equivalent TDP trees：**在 BERT-FT 不正确的情况下，有时会产生一个等价的或非常相似的树（由于重叠等关系是传递的，因此可能会有多个等价的树）。今后的工作可能包括开发一个更灵活的评分功能来解释这一点。

局限性：仍然难以解决句法歧义。例如：

**Example 2:** Foreign ministers **agreed** to set up a panel to investigate **who shot down** the Rwandan president’s plane on **April 6, 1994**.



# 5 **Conclusion and Future Work**

本文提出了两个模型，将 BERT 纳入时序依存解析器，并观察到与以前的方法相比的增益。我们分析了 BERT 在哪里和如何帮助这项具有挑战性的任务。对于未来的研究，计划探索其他类型的深层神经 LMS，如Transformer-XL (Dai et al., 2019) 和 XLNet (Yang et al., 2019)。还计划开发一个更灵活的评分函数，它可以处理等价的树。最后，计划在其他时间关系数据集上评估 BERT-FT，如 TempEval-3。