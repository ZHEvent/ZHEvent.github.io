---
layout: post
title: 2019 CoNLL《Deep Structured Neural Network for Event Temporal Relation Extraction》
categories: [Note, Temporal Relation]
description: 利用统计资源提高时序关系抽取
keywords: Temporal Relation, Deep SSVM
mathjax: true
original: true
author: 王亮
authorurl: https://github.com/NeoAlice
---

> 2019 CoNLL《基于深度结构化神经网络的事件时序抽取》[(Deep Structured Neural Network for Event Temporal Relation Extraction)](https://www.aclweb.org/anthology/K19-1062/)的阅读笔记

# **Abstract**

本文提出了一种新的事件时序关系抽取的深度结构化学习框架。 该模型由 1) RNN 学习成对关系的评分函数，2) SSVM 进行联合预测。神经网络自动学习考虑长期上下文的表示，为结构化模型提供健壮的特征，而 SSVM 则将域知识（如时序关系的传递闭包）作为约束，以做出更好的全局一致的决策。通过联合训练这两个组件，本模型结合了数据驱动学习和知识开发的好处。在三个高质量的事件时序关系数据集 (TCR、MATRES 和 TB-Dense) 上的实验结果表明，与预先训练的上下文化嵌入相结合，该模型在所有三个数据集上的性能明显优于最先进的方法。



# **1 Introduction**

事件时序关系抽取的目的是建立一个图，其中节点对应于给定文本中的事件，边表示事件之间的时序关系。如图 1a。

时序关系抽取有利于许多下游任务，如问答、信息检索和自然语言生成。一个事件图能够帮助时间序列预测和对自然语言生成提供指导。

时序关系抽取的一个主要挑战源于它是一个结构化预测问题的本质。 虽然关系图可以分解为每个事件对上的单个关系，但任何不被整个事件图所告知的局部模型通常都不会做出全局一致的预测，从而降低了整体性能。 图 1b 给出了一个例子，其中局部模型对被 **overruled** 和 **claiming** 之间的关系进行了不正确的分类，因为它只考虑成对的预测：考虑到 **filed** 和 **claiming** 之间的关系是同时发生的，因此违反了图的时间传递性约束。在图 1c 中，结构化模型改变了对 **overruled** 和 **claiming** 之间关系的预测，以确保所有预测的边缘类型的兼容性。



<div style="text-align: center;">
<img src="/images/blog/deep-ssvm-1.png" width="350px"/>
</div>



以往关于事件时序关系抽取的工作大多将其表述为一个成对的分类问题而无视全局结构。在训练过程中，有一些先前的工作直接模拟全局结构，然而，这些结构化模型依赖于使用语言规则和本地上下文的人工特征，这无法充分捕捉事件之间潜在的长期依赖关系。在图 1 所示的示例中，**filed** 发生在比 **overruled** 更早的上下文中。 因此，结合长期的上下文信息对于正确预测时间关系至关重要。

本文提出了一种新的深度结构化学习模型，以解决以往方法的不足。具体来说，我们调整了结构化支持向量机 (SSVM)，以纳入语言约束和领域知识，以便对事件时序关系进行联合预测。此外，用 RNNs 来增强这个框架，以学习长期的上下文。 尽管最近的工作成功地使用神经网络模型进行事件时序关系抽取 (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Mengand Rumshisky, 2018)，但这些系统没有利用问题结构进行两两预测。

我们开发了一个联合的端到端训练方案，使来自全局结构的反馈能够直接指导神经网络学习表示，从而允许我们的深度结构化模型结合数据驱动学习和知识开发的好处。在消融实验中，我们进一步证明了每个全局约束，语言特征的影响，以及在局部模型中使用上下文词表示的重要性。

综上所述，本文的贡献如下：

+ 提供了一个 deep SSVM model 抽取事件时序关系。
+ 在三个事件关系基准数据集上展示了最先进的结果。
+ 进行了广泛的消融研究和彻底的误差分析，以了解所提出的模型的能力和局限性，为今后的时序关系抽取研究提供了见解。



# 2 **Methods**

采用 Ning 的标注，R 表示关系集，$$\epsilon$$ 表示事件实体集。

## **Deep SSVM**

SSVM loss:

$$L=\sum \limits_{n=1}^l \frac{1}{M^n}[max(0,\Delta(y^n,\hat{y}^n)+S(\hat{y}^n;x^n)-S(y^n;x^n))]+C\Vert \Phi \Vert^2$$,			(1)

where $$\Phi$$ denotes model parameters, *n* indexes instances, $$M^n$$ is the number of event pairs in instance *n*. $$y^n,\hat{y}^n$$denote the gold and predicted global assignments for instance *n*, $$\Delta(y^n,\hat{y}^n)$$ is hamming distance, S is the scoring function to be learned.

和传统 SSVM 的区别是打分函数。传统 SSVM 使用线性函数，而 deep SSVM 使用 RNN。



## **RNN-Based Scoring Function**

我们引入了一个基于 RNN 的两两评分函数，以数据驱动的方式学习特征，并在输入中捕获长期上下文。局部神经结构是由 Tourille

et al. (2017b) 等人在实体关系抽取方面的先前工作所启发的。如图 2，输入层由词表示和输入句子中的每个 token 的 POS tag embeddings。其中，词表示由 BERT 获取且固定，而 POS tag embeddings 则在训练中被调整。将两者拼接来表示输入 token，然后输入到 Bi-LSTM 层中，得到上下文化的表示。



<div style="text-align: center;">
<img src="/images/blog/deep-ssvm-2.png" width="350px"/>
</div>



对于每个事件对 (i，j)，取每个事件对应的前向和后向隐藏向量，即 $$f_i,b_i,f_j,b_j$$ 来编码事件 token。然后将这些隐藏向量拼接起来形成对最终线性层的输入，在所有可能的成对关系上产生 softmax 分布，这就是 RNN-based 的评分函数。



## **Inference**

在训练过程和测试时间需要进行推理，以获得损失函数中的 $$\hat{y}^n$$，以及全局兼容的 assignments。 该推理框架是通过利用局部模型的分数和几个全局约束构造一个全局目标函数：对称性和传递性 (Bramsen et al. (2006b); Chambers and Jurafsky (2008); Denis and Muller (2011); Do et al.(2012); Ning et al. (2017); Han et al. (2019b)), 以及 Ning et al. (2018a)  提出的语言规则和时间因果约束确保全球一致性。 在本工作中，我们结合了对称性、传递性和时序因果约束。

+ **Objective Function.** 

  $$\hat{y}=arg\ max\sum\limits_{(i,j)\in\epsilon\epsilon}\sum\limits_{r\in R}y_{i,j}^rS(y_{i,j}^r;x)$$			(2)

  $$s.t.\ y_{i,j}^r\in\{0,1\},\sum\limits_{r\in R}y_{i,j}^r=1$$

+ **Symmetry and Transitivity constraint.** 

  $$\forall(i,j),(j,k)\in\epsilon\epsilon,y_{i,j}^r=y_{j,i}^{\overline{r}},$$					 （对称性）

  $$y_{i,j}^{r_1}+y_{j,k}^{r_2}-\sum\limits_{r3\in Trans(r_1,r_2)}y_{i,k}^{r_3}\leq1.$$			（传递性）

+ **Temporal-causal Constraint.**

  时序因果约束用于 TCR 数据集：

  $$y_{i,j}^c=y_{j,i}^{\overline{c}}\leq y_{i,j}^b$$

  where c and *c*¯correspond to the label *CAUSES* and *CAUSED BY*, and b represents the label *BEFORE*.

  也就是说，如果 i 导致了 j，那么 i 一定发生在 j 之前。



## **Learning**

我们开发了一种两步学习方法来优化神经 SSVM。首先训练本地评分函数，没有来自全局反馈约束。换句话说，局部神经网络模型在第一阶段只使用两两关系，通过最小化交叉熵损失进行优化。在第二阶段，切换到方程 (1) 中的全局目标函数，并重新优化网络以调整全局属性。 下面的章节将第一阶段的局部评分模型表示为局部模型，将最终模型表示为全局模型。



# 3 **Experimental Setup**

在这一部分中，描述使用的三个数据集。然后定义评价指标。最后，提供模型实现和实验的细节。



## **Data**

数据如表 1。



<div style="text-align: center;">
<img src="/images/blog/deep-ssvm-3.png" width="350px"/>
</div>



在所有三个数据集中，事件对总是按它们在文本中的出现顺序标注，即给定标记的事件对 (i，j)，事件 i 在文本中总是出现在事件 j 之前。且在训练和验证集中，将如果 (i,j) 对存在，就将 (j,i) 对也加入进去以加强数据。



## **Evaluation Metrics**

为了与基线模型中使用的评估度量相一致，采用两种稍微不同的度量计算。

+ **Micro-average**

  对于所有数据集，计算微观平均分数。对于密集标注的数据，微平均度量应该具有相同的精度、召回率和 F1 分数。然而，由于VAGUE 对被排除在 TCR 和 MATRES 的微均值计算中，以便与基线模型进行公平的比较，因此在报告这两个数据集的结果时，精度、召回和 F1 评分的微均值是不同的。

+ **Temporal Awareness (TE3)**

  对于 TB-Dense 数据集，采用 TE3 评估方案。 TE3 评分不仅考虑了正确对的数量，而且还捕捉了时序图是如何“有用”的。这里只考虑分数。



## **Implementation Details**

通过 Adam optimizer 最小化交叉熵来训练局部模型。使用预训练的 BERT 768 维 embedding 作为输入词表示，一层 MLP 作为分类层。结构化学习阶段，从 Adam optimize 转换成 SGD optimizer，通过 decay and momentum 参数观察性能提升。为了解决推理过程中的 ILP，使用 Gurobi optimizer。

超参数由验证集上的性能选择，超参数的最佳组合见表 2，我们对 3 种不同的随机种子进行了实验，并报告了平均结果。



<div style="text-align: center;">
<img src="/images/blog/deep-ssvm-4.png" width="350px"/>
</div>



对于 TCR 数据，需要一个单独的因果关系分类器。 由于少量的因果对，除了图 2 中的原始线性层，构建了一个独立的最终线性层。换句话说，有两个最终的线性层：当训练时序对或因果对时，它们中只有一个是活动的。



# 4  **Results and Analysis**



如图 3。

<div style="text-align: center;">
<img src="/images/blog/deep-ssvm-5.png" width="350px"/>
</div>



+ TCR

  

  <div style="text-align: center;">
  <img src="/images/blog/deep-ssvm-6.png" width="350px"/>
  </div>

  

+ MATRES

  

  <div style="text-align: center;">
  <img src="/images/blog/deep-ssvm-7.png" width="350px"/>
  </div>

  

+ TB-Dense

  

  <div style="text-align: center;">
  <img src="/images/blog/deep-ssvm-8.png" width="350px"/>
  </div>



# **Error Analysis**

为了理解为什么局部模型和结构化模型出错，随机抽样了 345 个案例中的 50 对，其中两个模型的预测在所有 3 个随机种子中都是不正确的。对这些对进行定性分析，并将它们分类为四种情况，如表 6 所示，每个情况与一个例子配对。



<div style="text-align: center;">
<img src="/images/blog/deep-ssvm-9.png" width="350px"/>
</div>



第一个案例说明，正确的预测需要更广泛的上下文知识。 例如，**transition** 和 **discuss** 的黄金标签是 BEFORE，其中名义事件 **transition** 指的是历史上的一个在第二句中 **discuss** 之前结束的特定时期。人类标注者可以根据他们在历史上的知识很容易地推断出这种关系，但这对于没有先验知识的机器是很困难的。我们认为这是一个非常常见的错误，特别名义事件对。第二种情况表明，否定可以完全改变时间顺序。**planned** 和 **plans** 事件对的黄金标签是 After，因为否定标志 **no** 无限期地推迟了 **planned** 事件。模型不接收这个信号，因此预测为 VAGUE。

最后， “intention”  事件可能使时序关系预测变得困难  (Ning et al., 2018b)。案例 3 表明，我们的模型可以忽略 “intention”  标记，例如例子中的 **aimed at**，从而在 **doubling** 和 **signed** 之间做出不正确的预测 VAGUE，而真正的标签是 AFTER，因为 **doubling** 是一个没有发生的 “intention” 。



# **5 Ablation Studies**

## **Effect of the structured constraints**

关于对称约束的一个简单的消融研究是从全局推理过程中删除它。然而，尽管在全局推理中显式地消除了对称约束，但它在数据增强步骤中被隐式地使用。为了更好地理解对称约束的好处，我们研究了显式应用对称约束在 SSVM 中的贡献以及它在数据增强中的隐式影响。

因此，将具有原始顺序和翻转顺序的一对视为不同的学习和评估实例。将原始顺序的对表示为“前向”数据，它们的翻转顺序对应表示为“后向”数据，它们的组合表示为“双向”数据。

训练了四个额外的模型来研究对称性和传递性约束的影响：1) 前向数据上训练的局部模型；2) 前向数据上训练的具有传递性约束的全局模型；3) 双向数据上训练的局部模型；4) 双向数据上训练的具有传递性约束的全局模型，分别表示为 M1，M2，M3，M4。M1 和 M2 是不应用任何对称性质的模型；M3 和 M4 是隐式利用对称性质的模型。

此外，如果删除对称约束，则应该重新定义评估设置。在先前工作的标准评估设置中，评估只在文本中与其原始顺序（前向数据）的对上执行。这种评估假设一个模型将在前向和后向数据上同等地良好运作，当显式地施加对称约束时，这当然是成立的。然而，正如我们在后面的分析中所观察到的，当删除对称约束时，这个假设失败了。为了证明模型对后向数据的鲁棒性的提高，我们提出在前向数据和双向数据上对模型进行测试。 如果一个模型是稳健的，它应该在这两种情况下都表现良好。



<div style="text-align: center;">
<img src="/images/blog/deep-ssvm-10.png" width="350px"/>
</div>



对表 7 结果的分析如下：

+ 传递性的影响：通过将 M1 与 M2 和 M3 与 M4 进行比较，所有三个数据集的一致改进表明了全局传递性约束的有效性。
+ 隐式对称性的影响（数据增强）：检查 M1 和 M3 以及 M2 和 M4 之间的对比，可以看到在双向评估中有显著的改进，尽管前向评估的性能略有下降。这些比较意味着数据增强可以帮助提高模型的鲁棒性。 
+ 显式对称的影响：通过将所提出的模型与 M4 进行比较，所有数据集的一致改进证明了使用显式对称特性的好处。

+ 模型鲁棒性：虽然 M1 和 M2 在前向测试数据上显示出竞争结果，但它们的性能在双向评估中显著下降。相反，该模型在两种测试场景中都取得了较强的性能(除一个最佳 F1 分数外)，从而证明了该方法的鲁棒性。



## **Effect of linguistic features**

先前的研究证实了在事件关系预测中利用语言特征的成功。利用上下文化的词嵌入的一个优点是提供丰富的语义表示，并可能避免使用额外的语言特征。在这里，我们通过使用原始数据集中提供的简单特征：token 距离、时态和事件实体的极性，来研究将语言特征纳入模型的影响。这些特征与线性层之前的 Bi-LSTM 隐藏状态(即图 2 中的 $$f_i,b_i,f_j,b_j$$)。表 8 分别显示了使用或不使用语言特征的本地和全局模型的 F1 分数。这些额外的特性可能导致过度拟合，因此不能提高测试的所有三个数据集的模型性能。这组实验表明，语言特征并不能提高我们当前框架的预测能力。



<div style="text-align: center;">
<img src="/images/blog/deep-ssvm-11.png" width="350px"/>
</div>



## **Effect of BERT representations**

本节探讨在我们的 deep SSVM 框架下上下文化的 BERT 表示的影响。将 BERT 表示替换为 GloVe 单词嵌入。表 9 分别显示了我们使用BERT 和 GloVe 的本地模型和全局模型的 F1 分数。BERT 非常显著地提高了性能。此外，即使没有 BERT 表示，我们基于 RNN 的局部模型和深度结构化全局模型仍然优于 (MATRES和TCR) 或与 (TB-Dense)的目前 SOTA 相当。这些结果证实了我们方法的改进。

.

<div style="text-align: center;">
<img src="/images/blog/deep-ssvm-12.png" width="350px"/>
</div>



# 6 **Conclusion**

在本文中，我们提出了一种新的基于 SSVM 的深度结构化模型，它结合了结构化模型编码结构知识的能力和数据驱动的深度神经结构学习远程特征的能力的好处。实验结果显示了这种方法在事件时序关系抽取中的有效性。

一个有趣的未来方向是进一步利用常识知识、时序关系领域知识和语言学信息，为结构化学习创造更健壮和全面的全局约束。 另一个方向是通过设计新的神经结构来改进特征表示，这些结构需要能够更好地捕捉错误分析中讨论的否定和假设短语。我们计划利用大量未标注的语料库来帮助事件时序关系的抽取。
