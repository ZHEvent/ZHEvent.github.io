---
layout: post
title: 2020 EMNLP《Severing the Edge Between Before and After:Neural Architectures for Temporal Ordering of Events》
categories: [Note, Temporal Relation, SMTL]
description: 切开之前和之后间的边：事件时序关系神经结构
keywords: Temporal Relation, Scheduled Multitask-Learning
mathjax: true
original: true
author: 王亮
authorurl: https://github.com/NeoAlice
---

> 2020 EMNLP《切开之前和之后间的边：事件时序关系神经结构》[(Severing the Edge Between Before and After:Neural Architectures for Temporal Ordering of Events)](https://www.aclweb.org/anthology/2020.emnlp-main.436/)的阅读笔记

# **Abstract**

本文提出一种神经结构和一组通过预测时序关系来排序事件的训练方法。提出的模型以一个文本内的一对事件及其区间作为输入，识别它们之间的时序关系。鉴于这项任务的一个关键挑战是标注数据的稀缺性，模型依赖于预训练的表示  (i.e. RoBERTa, BERT or ELMo)，迁移和多任务学习（通过利用互补数据集），以及自训练技术。 在英文文档的 MATRES 数据集上的实验为这项任务建立了新的 SOTA。



# **1 Introduction**

时序关系预测任务具有挑战性，因为它需要深入理解语言的时间方面而且标注数据的数据量很少。

贡献：

+ 一种神经结构，在训练数据缺乏的情况下，可以灵活地适应不同的编码器和预训练的单词嵌入器，形成上下文成对论元的表示。
+ 探索了现有的调度多任务学习框架 (Scheduled Multitask-Learning ) 的应用 (Kiperwasser and Ballesteros, 2018) ，利用互补（时间和非时间）信息到我们的模型；这模仿了预训练和微调。
+ 一种自训练方法，结合了我们模型的预测并从中学习；与 SMTL 方法联合测试它。

baseline 使用 RoBERTa (Liu et al., 2019b) ，其结果已经超出 SOTA 2 个点，应用 SMTL 技术可以进一步改进我们的至少一个辅助任务。 最后，自训练实验，利用SMTL，建立了另一个 SOTA，产生了几乎 4 个 F1 点的总改进。



# **2 Our Baseline Model**

n 个 tokens，以及 2 个事件的 span ([start,end]) 作为输入。

首先，将输入序列进行词嵌，然后这些词嵌随机用 LSTM 或 Transformers 编码（对于 BERT 和 RoBERTa 不需要这步）。

为了获取上下文信息，按照 2 个事件的 start,end 分成 5 个子序列。即 $$S_1:H_{[0,start_1]},S_2:H_{[start_1,end_1]},S_3:H_{[end_1,start_2]},S_4:H_{[start_2,end_2]},S_5:H_{[end_2,n]}$$ ,并对这些序列 *attention* pooling 和 mean pooling。将其拼接后得到上下文表示，最后通过全连接层和 softmax 函数得到结果。整个模型使用交叉熵损失进行端到端训练。



# 3 **Scheduled Multi-Task Learning**

参见Eliyahu Kiperwasser and Miguel Ballesteros. 2018.Scheduled multi-task learning: From syntax to translation. *Transactions of the Association for Computational Linguistics*, 6:225–240.

SMTL 是一种半监督的学习方法，将多任务学习 Multi-task learning, Pre-training, and Fine-tuning 结合。

+ Multi-Task Learning

  通过最大化共享参数的数量解决多个相互促进的任务。主要受益于表示偏差、从其他任务的信息中获得线索。但是对于任务间相对独立的特征，强行拼凑会导致性能不如单任务学习。

+ Pre-Training

  如 BERT.

+ Fine-Tuning 

  使用小数量的域内语料和大量域外语料评估参数。一般是在预训练神经模型的最后一层进行微调。

SMTL 加入了调度器，来从任务队列选择下一轮训练的样本。



<div style="text-align: center;">
<img src="/images/blog/smtl-2.png" width="350px"/>
</div>



# 4 Multi-task Learning

由于标注数据较少，通过 SMTL 从其他互补任务中获得信息以扩充模型。



## **Method**

同 Kiperwasser and Ballesteros (2018)，使用三个调度器，分别为常数、sigmoid、指数曲线 p(t)，其是指从主任务中取一个 batch 的概率，t 是训练过程中累计的已访问数据量。α 是超参数。常数调度器把 batches 随机划分。每一个步骤，模型都会训练主任务或者辅助任务的句子（$$p^{const}(t)=\alpha,0\leq\alpha\leq1$$）（note:这种调度器同普通的 MTL）。sigmoid 调度器允许模型在开始时访问主任务和辅助任务的 batch，而最新的更新只能是主任务的 batch（$$p^{sig}(t)=\frac{1}{1+e^{-\alpha t}}$$）。指数调度器则是开始时只能辅助任务，最新的更新只能是主任务（$$p^{exp}(t)=1-e^{\alpha t}$$）。

同过去的工作，将经过训练的任务向量预先发送到编码器，以帮助模型区分主任务和辅助任务。



<div style="text-align: center;">
<img src="/images/blog/smtl-3.png" width="350px"/>
</div>



## **Auxiliary Datasets**

在 SMTL 中使用三种辅助数据集。前两张和 MATRES 的分类和标签不同，但有 gold label，最后一种是 silver dataset （只有预测的 label）但是和 MATRES 标记相同。

第一个是 ACE ，假设该任务可以增加不同领域的知识和文本中给定关系分类链接两个 span 的概念知识。虽然这与事件和最终任务在相似性方面没有直接关系，但对的 span 分类是包含 ACE 的原因。

更接近和互补的时序标注数据集 **Timebank  and Aquaint annotations** (timex-event, event-timex, timex-timex) 如图 2.

用自训练方法得到第三个 **silver** 数据集。需要未标记的文本，从文本中抽取事件的标记器，和一个分类器预测时序关系。使用来自 CNN /每日邮件数据集的 6000个随机文档，在这些文档中选择了 85K 段文本，这些文本在分词后包含 10 到 40 个 token。训练一个基于 RoBerta 的命名实体标记器，并使用它来标记这些段中的事件。生成了大约 65K 事件。对于总计 285K 事件对，位于一个段内，作为时间排序的候选。 最后，使用基线 Roberta 时间模型对这些候选对之间的时间关系进行分类，并使用基于 Softmax 分数的前 三分之二 位分类来获得大约 190K 的 silver 关系实例。



# 5 Experiments and Results

MATRES 是主要的训练和验证数据集。



<div style="text-align: center;">
<img src="/images/blog/smtl-1.png" width="350px"/>
</div>



ACE 上的性能较差，可能是由于该任务同 MATRES 差别较大导致。



# 6 Conclusion

本文提出了用于时序事件的神经结构。通过预训练，SMTL 和自训练技术，利用互补任务，建立了一个新的技术。未来的工作，可以通过对最好的自训练模型产生的数据进行再训练来运行几次迭代，而不是使用 RoBERTa 基线模型进行自我训练实验；这可能是进一步改进的途径。此外计划通过使用跨语言模型将工作扩展到英语以外的其他语言（由于缺乏数据，目前还没有尝试过），应用 CNNs 等其他体系结构，将树结构纳入模型，或者联合执行事件识别和时序。

